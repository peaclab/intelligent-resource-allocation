{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db36fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and numerical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook import\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15ebc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column headers, extracted from the \"man 5 accounting\" command:\n",
    "col_headers=[\"qname\", \"hostname\", \"group\", \"owner\", \"job_name\", \"job_number\", \"account\", \"priority\", \n",
    "             \"submission_time\", \"start_time\", \"end_time\", \"failed\", \"exit_status\", \"ru_wallclock\", \n",
    "             \"ru_utime\", \"ru_stime\", \"ru_maxrss\", \"ru_ixrss\", \"ru_ismrss\", \"ru_idrss\", \"ru_isrss\", \n",
    "             \"ru_minflt\", \"ru_majflt\", \"ru_nswap\", \"ru_inblock\", \"ru_oublock\", \"ru_msgsnd\", \n",
    "             \"ru_msgrcv\", \"ru_nsignals\", \"ru_nvcsw\", \"ru_nivcsw\", \"project\", \"department\", \"granted_pe\", \n",
    "             \"slots\", \"task_number\", \"cpu\", \"mem\", \"io\", \"category\", \"iow\", \"pe_taskid\", \"maxvmem\", \n",
    "             \"arid\", \"ar_submission_time\"]\n",
    "\n",
    "use_col_headers = [\"group\", \"owner\", \"job_name\", \"account\", \n",
    "             \"submission_time\", \"start_time\", \"end_time\", \"failed\", \"exit_status\", \"ru_wallclock\", \n",
    "             \"ru_utime\", \"ru_stime\", \"ru_maxrss\", \"ru_isrss\", \"project\", \"granted_pe\", \n",
    "             \"slots\", \"cpu\", \"mem\", \"io\", \"category\"]\n",
    "\n",
    "# Input file path\n",
    "file_path = \"/projectnb/peaclab-mon/boztop/resource-allocation/datasets/bu scc accounting/bu.accounting.2023\" \n",
    "\n",
    "# Reading the utf-8 encoded CSV file\n",
    "df = pd.read_csv(file_path, encoding='utf-8', skiprows=4, \n",
    "                 names=col_headers,  sep=':', usecols = use_col_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc7af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows where wallclock is 0\n",
    "df = df[df['ru_wallclock'] != 0]\n",
    "\n",
    "# Keep the successful jobs only\n",
    "df = df[df['failed'] == 0]\n",
    "df = df[df['exit_status'] == 0]\n",
    "\n",
    "# Add a column for the lag time\n",
    "df['lag_time'] = df['start_time'] - df['submission_time']\n",
    "\n",
    "# Remove any rows where this new column is <= 0\n",
    "df = df[df['lag_time'] > 0]\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ed2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['submission_time'] = df['submission_time'].apply(lambda x: datetime.datetime.fromtimestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905fa0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional metrics\n",
    "df['execution_time'] = df['end_time'] - df['start_time']\n",
    "\n",
    "# CPU-related\n",
    "# df['total_cpu_time'] = df['ru_utime'] + df['ru_stime']\n",
    "# df['ncpu'] = np.ceil(df['total_cpu_time'] / df['ru_wallclock'])\n",
    "df['ncpu'] = np.ceil(df['cpu'] / df['ru_wallclock'])\n",
    "df['cpu_waste'] = df['slots'] - df['ncpu']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2caab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary stats\n",
    "all_stats = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50141237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of jobs: 11403153\n",
      "Total number of different groups: 524\n",
      "Total number of different users: 1388\n",
      "Total number of jobs where the request was more than actual usage: 2299636\n",
      "Percentage of jobs where the request was more than actual usage: 20.1666679382448 %\n",
      "Total CPU wastage (number of cores): 16058705.0\n"
     ]
    }
   ],
   "source": [
    "# The total number of jobs\n",
    "njobs = int(all_stats['ru_wallclock']['count'])\n",
    "print(f'Total number of jobs: {njobs}')\n",
    "\n",
    "# The total number of project groups\n",
    "total_groups = df['group'].nunique()\n",
    "print(f\"Total number of different groups: {total_groups}\")\n",
    "\n",
    "# The total number of users\n",
    "total_users = df['owner'].nunique()\n",
    "print(f\"Total number of different users: {total_users}\")\n",
    "\n",
    "# The total number of jobs where the request was more than actual usage\n",
    "jobs_with_request_more_than_usage = (df['slots'] > df['ncpu']).sum()\n",
    "print(f'Total number of jobs where the request was more than actual usage: {jobs_with_request_more_than_usage}')\n",
    "\n",
    "percentage_waste = 100 * (jobs_with_request_more_than_usage/njobs)\n",
    "print(f'Percentage of jobs where the request was more than actual usage: {percentage_waste} %')\n",
    "\n",
    "\n",
    "# The total CPU core wastage\n",
    "total_cpu_wastage = df['cpu_waste'].sum()\n",
    "print(f\"Total CPU wastage (number of cores): {total_cpu_wastage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05ba7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>job_name</th>\n",
       "      <th>ru_wallclock</th>\n",
       "      <th>ru_maxrss</th>\n",
       "      <th>ru_utime</th>\n",
       "      <th>ru_stime</th>\n",
       "      <th>slots</th>\n",
       "      <th>ncpu</th>\n",
       "      <th>cpu_waste</th>\n",
       "      <th>cpu</th>\n",
       "      <th>mem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>piotrt</td>\n",
       "      <td>caltech101_erm_5e-5-0-v3_2-eval</td>\n",
       "      <td>14</td>\n",
       "      <td>3468304.0</td>\n",
       "      <td>10.508114</td>\n",
       "      <td>3.146432</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.654546</td>\n",
       "      <td>113.041253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>piotrt</td>\n",
       "      <td>hospital3_erm_5e-5-0-v3_3-nosma-full</td>\n",
       "      <td>15585</td>\n",
       "      <td>3567556.0</td>\n",
       "      <td>26864.717200</td>\n",
       "      <td>5754.854540</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32619.571740</td>\n",
       "      <td>782537.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>piotrt</td>\n",
       "      <td>caltech101_erm_5e-5-0-v3_2-nosma-eval</td>\n",
       "      <td>13</td>\n",
       "      <td>3470784.0</td>\n",
       "      <td>10.539715</td>\n",
       "      <td>3.114007</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.653722</td>\n",
       "      <td>108.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>drhall</td>\n",
       "      <td>sge.jcf</td>\n",
       "      <td>94</td>\n",
       "      <td>133992.0</td>\n",
       "      <td>90.256206</td>\n",
       "      <td>2.717040</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>92.973246</td>\n",
       "      <td>16.411151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>piotrt</td>\n",
       "      <td>caltech101_erm_5e-5-0-v3_2-nosma-full-eval</td>\n",
       "      <td>14</td>\n",
       "      <td>3481124.0</td>\n",
       "      <td>10.547930</td>\n",
       "      <td>3.138102</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.686032</td>\n",
       "      <td>118.503044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689662</th>\n",
       "      <td>farrell</td>\n",
       "      <td>nf-haplotype_caller_(61968)</td>\n",
       "      <td>5855</td>\n",
       "      <td>2958472.0</td>\n",
       "      <td>11606.093233</td>\n",
       "      <td>60.866751</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11666.959984</td>\n",
       "      <td>115496.741931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689663</th>\n",
       "      <td>gychuang</td>\n",
       "      <td>sge.jcf</td>\n",
       "      <td>1072</td>\n",
       "      <td>18392.0</td>\n",
       "      <td>1066.370101</td>\n",
       "      <td>0.551287</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1066.921388</td>\n",
       "      <td>105.564728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689667</th>\n",
       "      <td>okhan01</td>\n",
       "      <td>map-3n7a_L</td>\n",
       "      <td>307</td>\n",
       "      <td>1136084.0</td>\n",
       "      <td>6705.686539</td>\n",
       "      <td>403.934059</td>\n",
       "      <td>28</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7109.620598</td>\n",
       "      <td>12670.641529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689669</th>\n",
       "      <td>okhan01</td>\n",
       "      <td>map-3n7a_L</td>\n",
       "      <td>313</td>\n",
       "      <td>1032180.0</td>\n",
       "      <td>6812.607501</td>\n",
       "      <td>169.738650</td>\n",
       "      <td>28</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6982.346151</td>\n",
       "      <td>12418.058213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689671</th>\n",
       "      <td>farrell</td>\n",
       "      <td>nf-haplotype_caller_(62192)</td>\n",
       "      <td>3325</td>\n",
       "      <td>2367500.0</td>\n",
       "      <td>6053.444509</td>\n",
       "      <td>499.328084</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6552.772593</td>\n",
       "      <td>63152.861747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2299636 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             owner                                    job_name  ru_wallclock  \\\n",
       "1           piotrt             caltech101_erm_5e-5-0-v3_2-eval            14   \n",
       "5           piotrt        hospital3_erm_5e-5-0-v3_3-nosma-full         15585   \n",
       "11          piotrt       caltech101_erm_5e-5-0-v3_2-nosma-eval            13   \n",
       "13          drhall                                     sge.jcf            94   \n",
       "18          piotrt  caltech101_erm_5e-5-0-v3_2-nosma-full-eval            14   \n",
       "...            ...                                         ...           ...   \n",
       "13689662   farrell                 nf-haplotype_caller_(61968)          5855   \n",
       "13689663  gychuang                                     sge.jcf          1072   \n",
       "13689667   okhan01                                  map-3n7a_L           307   \n",
       "13689669   okhan01                                  map-3n7a_L           313   \n",
       "13689671   farrell                 nf-haplotype_caller_(62192)          3325   \n",
       "\n",
       "          ru_maxrss      ru_utime     ru_stime  slots  ncpu  cpu_waste  \\\n",
       "1         3468304.0     10.508114     3.146432      4   1.0        3.0   \n",
       "5         3567556.0  26864.717200  5754.854540      4   3.0        1.0   \n",
       "11        3470784.0     10.539715     3.114007      4   2.0        2.0   \n",
       "13         133992.0     90.256206     2.717040     28   1.0       27.0   \n",
       "18        3481124.0     10.547930     3.138102      4   1.0        3.0   \n",
       "...             ...           ...          ...    ...   ...        ...   \n",
       "13689662  2958472.0  11606.093233    60.866751      3   2.0        1.0   \n",
       "13689663    18392.0   1066.370101     0.551287      8   1.0        7.0   \n",
       "13689667  1136084.0   6705.686539   403.934059     28  24.0        4.0   \n",
       "13689669  1032180.0   6812.607501   169.738650     28  23.0        5.0   \n",
       "13689671  2367500.0   6053.444509   499.328084      3   2.0        1.0   \n",
       "\n",
       "                   cpu            mem  \n",
       "1            13.654546     113.041253  \n",
       "5         32619.571740  782537.612000  \n",
       "11           13.653722     108.458730  \n",
       "13           92.973246      16.411151  \n",
       "18           13.686032     118.503044  \n",
       "...                ...            ...  \n",
       "13689662  11666.959984  115496.741931  \n",
       "13689663   1066.921388     105.564728  \n",
       "13689667   7109.620598   12670.641529  \n",
       "13689669   6982.346151   12418.058213  \n",
       "13689671   6552.772593   63152.861747  \n",
       "\n",
       "[2299636 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the cpu wasted jobs\n",
    "\n",
    "df_cpu = df[df['cpu_waste'] > 0]\n",
    "\n",
    "df_cpu[['owner', 'job_name', 'ru_wallclock', 'ru_maxrss', 'ru_utime','ru_stime', 'slots', 'ncpu', 'cpu_waste','cpu', 'mem']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts = df.groupby('owner').size()\n",
    "users_with_enough_data = row_counts[row_counts >= 10].index\n",
    "filtered_df = df[df['owner'].isin(users_with_enough_data)]\n",
    "\n",
    "top_10_users = filtered_df.groupby('owner')['cpu_waste'].mean().sort_values(ascending=False).head(10)\n",
    "\n",
    "\n",
    "# Count the number of rows for each of these top 10 users\n",
    "row_counts = df[df['owner'].isin(top_10_users.index)].groupby('owner').size()\n",
    "\n",
    "# Convert the result to a DataFrame for a cleaner display\n",
    "row_counts_df = row_counts.reset_index(name='row_count')\n",
    "\n",
    "\n",
    "# Display the group names along with their row counts\n",
    "print(row_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = df[df['owner'] == 'hrhiginb'][['cpu','ru_wallclock','slots', 'ncpu']]\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58327883",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from waste_visualize import waste_visualize\n",
    "waste_visualize(df, 'owner')\n",
    "waste_visualize(df, 'group')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from statistical_analysis import process_user_data\n",
    "# Select the last \"x\" entries\n",
    "df_last_1500 = df.tail(50000)\n",
    "\n",
    "# Process data for each user with a step size of 3\n",
    "user_dfs, accuracy_results = process_user_data(df_last_1500, 'ncpu', 3)\n",
    "\n",
    "# Print the results\n",
    "for user, (mae, mape, row_count) in accuracy_results.items():\n",
    "    print(f\"Accuracy for user {user}: MAE = {mae}, MAPE = {mape}%, Rows = {row_count}\")\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ----\n",
    "\n",
    "def custom_loss(alpha, beta):\n",
    "    def loss(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        squared_error = tf.square(error)\n",
    "        \n",
    "        weighted_error = tf.where(error > 0, alpha * squared_error, beta * squared_error)\n",
    "        return tf.reduce_mean(weighted_error)\n",
    "    return loss\n",
    "\n",
    "def dtw_distance(s1, s2):\n",
    "    \"\"\"Calculate Dynamic Time Warping (DTW) distance between two time series.\"\"\"\n",
    "    n, m = len(s1), len(s2)\n",
    "    dtw = np.zeros((n + 1, m + 1))\n",
    "    \n",
    "    for i in range(n + 1):\n",
    "        for j in range(m + 1):\n",
    "            dtw[i, j] = float('inf')\n",
    "    \n",
    "    dtw[0, 0] = 0\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = abs(s1[i - 1] - s2[j - 1])\n",
    "            dtw[i, j] = cost + min(dtw[i - 1, j],    # Insertion\n",
    "                                   dtw[i, j - 1],    # Deletion\n",
    "                                   dtw[i - 1, j - 1])  # Match\n",
    "    \n",
    "    return dtw[n, m]\n",
    "\n",
    "def dtw_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate Dynamic Time Warping (DTW) score between true and predicted values.\"\"\"\n",
    "    return dtw_distance(y_true, y_pred)\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error (MAE) between true and predicted values.\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error (MAPE) between true and predicted values.\"\"\"\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    \n",
    "    dtw_score = dtw_metric(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'DTW': dtw_score,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# --- Plotting Graphs ----\n",
    "def draw_combined_scatter_plot(y_test, lstm_y_pred, cnn_y_pred, bnn_y_pred, y_requested, owner):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.scatter(range(len(y_test)), y_test, label='True CPU Usage', color='blue', marker='o')\n",
    "    plt.scatter(range(len(lstm_y_pred)), lstm_y_pred, label='LSTM Predicted CPU Usage', color='orange', marker='x')\n",
    "    plt.scatter(range(len(cnn_y_pred)), cnn_y_pred, label='CNN Predicted CPU Usage', color='green', marker='^')\n",
    "    plt.scatter(range(len(bnn_y_pred)), bnn_y_pred, label='BNN Predicted CPU Usage', color='red', marker='s')\n",
    "    plt.scatter(range(len(y_requested)), y_requested, label='Requested CPU cores', color='purple', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Job submissions')\n",
    "    plt.ylabel('CPU Usage')\n",
    "    plt.title(f'User {owner}: Machine Learning Model Predictions vs True Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def draw_combined_prediction_results(y_test, lstm_y_pred, cnn_y_pred, bnn_y_pred, y_requested, owner):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.plot(y_test, label='True CPU Usage', color='blue')\n",
    "    plt.plot(lstm_y_pred, label='LSTM Predicted CPU Usage', color='orange')\n",
    "    plt.plot(cnn_y_pred, label='CNN Predicted CPU Usage', color='green')\n",
    "    plt.plot(bnn_y_pred, label='BNN Predicted CPU Usage', color='red')\n",
    "    plt.plot(y_requested, label='Requested CPU cores', color='purple', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Job submissions')\n",
    "    plt.ylabel('CPU Usage')\n",
    "    plt.title(f'User {owner}: Machine Learning Model Predictions vs True Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def draw_prediction_results(y_test, y_pred, y_requested, title):    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(y_test, label='True Values', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted Values', color='orange')\n",
    "    plt.plot(y_requested, label='True Values', color='green', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Job submissions')\n",
    "    plt.ylabel('CPU Usage')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Histogram time!!\n",
    "def plot_metrics_histogram(results):\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    bar_width = 0.2\n",
    "    index = np.arange(len(results_df['owner'].unique()))\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.bar(index - bar_width, results_df.groupby('owner')['cores_less_than_requested_percent'].mean(), bar_width, label='Cores Used < Requested Slots', color='red')\n",
    "\n",
    "    models = ['LSTM', 'CNN', 'BNN']\n",
    "    for i, model_name in enumerate(models):\n",
    "        model_results = results_df[results_df['model'] == model_name]\n",
    "        offset = (i - 1) * bar_width  # Offset for grouping bars\n",
    "\n",
    "        plt.bar(index + offset, model_results['cores_saved_percent'], bar_width, label=f'{model_name} Predictions')\n",
    "    \n",
    "    plt.xlabel('Users')\n",
    "    plt.ylabel('Percentage of Core Saved Job Predictions')\n",
    "    plt.title('Reducing CPU Waste for Different Users with ML Models')\n",
    "    plt.xticks(index + bar_width, results_df['owner'].unique(), rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results table\n",
    "    print(\"\\nResults Table:\")\n",
    "    print(results_df[['owner', 'model', 'cores_less_than_requested_percent', 'cores_saved_percent']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Creating Machine Learning Models -----\n",
    "\n",
    "# --- LSTM ----\n",
    "\n",
    "def build_lstm_model(input_shape, alpha, beta):\n",
    "    lstm_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "    ])\n",
    "    lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=custom_loss(alpha, beta),\n",
    "                  metrics=['mean_squared_error'])\n",
    "    return lstm_model\n",
    "\n",
    "# --- BNN ----\n",
    "\n",
    "def build_bnn_model(input_shape, alpha, beta):\n",
    "    bnn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "    ])\n",
    "    bnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=custom_loss(alpha, beta),\n",
    "                  metrics=['mean_squared_error'])\n",
    "    return bnn_model\n",
    "\n",
    "\n",
    "# --- CNN ----\n",
    "\n",
    "def build_cnn_model(input_shape, alpha, beta):\n",
    "    cnn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "    ])\n",
    "    cnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                      loss=custom_loss(alpha, beta),\n",
    "                      metrics=['mean_squared_error'])\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Data Preprocessing for ML -----\n",
    "\n",
    "def preprocess_data(group_data, lag=3):\n",
    "    \"\"\"\n",
    "    Preprocess the data by generating lag features, aligning requested CPU data,\n",
    "    splitting into train/test datasets, and reshaping the data.\n",
    "\n",
    "    Parameters:\n",
    "    - group_data (pd.DataFrame): The input dataframe containing 'ncpu' and 'slots' columns.\n",
    "    - lag (int): The number of lag features to generate.\n",
    "\n",
    "    Returns:\n",
    "    - X_train (np.ndarray): Training data features.\n",
    "    - X_test (np.ndarray): Testing data features.\n",
    "    - y_train (np.ndarray): Training data target values.\n",
    "    - y_test (np.ndarray): Testing data target values.\n",
    "    - requested_resource_data (np.ndarray): Requested resource data for the test set.\n",
    "    \"\"\"\n",
    "    lag_features = []\n",
    "    for i in range(1, lag + 1):\n",
    "        group_data[f'ncpu_lag_{i}'] = group_data['ncpu'].shift(i)\n",
    "        lag_features.append(f'ncpu_lag_{i}')\n",
    "\n",
    "    # Drop rows with NaN resulting from the lag\n",
    "    group_data.dropna(inplace=True)\n",
    "\n",
    "    # Align the requested CPU column's data\n",
    "    group_data['slots'] = group_data['slots'].shift(lag)\n",
    "    group_data.dropna(inplace=True)\n",
    "\n",
    "    X = group_data[lag_features].values\n",
    "    y = group_data['ncpu'].values\n",
    "\n",
    "    # print(f\"Number of lag features: {len(lag_features)}\")\n",
    "    # print(f\"Number of ncpu features: {len(y)}\")\n",
    "\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    requested_resource_data = group_data['slots'].values[split:]\n",
    "\n",
    "    # Reshape data\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, requested_resource_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779febad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Initialize lists to store metrics for each group\n",
    "underpredictions_list = []\n",
    "overpredictions_list = []\n",
    "cores_less_than_requested_list = []\n",
    "cores_saved_list = []\n",
    "group_names = []\n",
    "\n",
    "# Custom loss parameters\n",
    "alpha = 1.0\n",
    "beta = 1.0 \n",
    "\n",
    "for owner in top_10_users.index: # Apply the ML models for all 10 groups\n",
    "    print(f\"Processing user: {owner}\")\n",
    "    group_data = df[df['owner'] == owner].copy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, requested_resource_data = preprocess_data(group_data, lag=3)\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    bias = np.mean(y_train)\n",
    "\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # LSTM Model\n",
    "    lstm_model = build_lstm_model(input_shape, alpha, beta)\n",
    "    lstm_history = lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "                                 validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    lstm_y_pred_without_bias = lstm_model.predict(X_test).flatten()\n",
    "    lstm_y_pred = lstm_y_pred_without_bias + bias\n",
    "    mse_lstm = mean_squared_error(y_test, lstm_y_pred)\n",
    "    \n",
    "    # CNN Model\n",
    "    cnn_model = build_cnn_model(input_shape, alpha, beta)\n",
    "    cnn_history = cnn_model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "                               validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    cnn_y_pred_without_bias = cnn_model.predict(X_test).flatten()\n",
    "    cnn_y_pred = cnn_y_pred_without_bias + bias\n",
    "    mse_cnn = mean_squared_error(y_test, cnn_y_pred)\n",
    "    \n",
    "    # BNN Model\n",
    "    bnn_model = build_bnn_model(input_shape, alpha, beta)\n",
    "    bnn_history = bnn_model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "                               validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    bnn_y_pred_without_bias = bnn_model.predict(X_test).flatten()\n",
    "    bnn_y_pred = bnn_y_pred_without_bias + bias\n",
    "    mse_bnn = mean_squared_error(y_test, bnn_y_pred)\n",
    "    \n",
    "    # Calculate some more informative metrics\n",
    "    for y_pred, mse, model_name in zip([lstm_y_pred, cnn_y_pred, bnn_y_pred],\n",
    "                                        [mse_lstm, mse_cnn, mse_bnn],\n",
    "                                        ['LSTM', 'CNN', 'BNN']):\n",
    "        underpredictions = np.sum(y_test > y_pred)\n",
    "        overpredictions = np.sum(y_test < y_pred)\n",
    "        cores_less_than_requested = np.sum(y_test < requested_resource_data)\n",
    "        cores_saved = np.sum((y_pred < requested_resource_data) & (y_pred > y_test))\n",
    "\n",
    "        total_samples = len(y_test)  # Total number of test samples\n",
    "\n",
    "        # Calculate percentages\n",
    "        underpredictions_percent = (underpredictions / total_samples) * 100\n",
    "        overpredictions_percent = (overpredictions / total_samples) * 100\n",
    "        cores_less_than_requested_percent = (cores_less_than_requested / total_samples) * 100\n",
    "        cores_saved_percent = (cores_saved / total_samples) * 100\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            'owner': owner,\n",
    "            'model': model_name,\n",
    "            'underpredictions_percent': underpredictions_percent,\n",
    "            'overpredictions_percent': overpredictions_percent,\n",
    "            'cores_less_than_requested_percent': cores_less_than_requested_percent,\n",
    "            'cores_saved_percent': cores_saved_percent,\n",
    "            'mse': mse\n",
    "        })\n",
    "\n",
    "    # Plot the results\n",
    "    draw_combined_prediction_results(y_test, lstm_y_pred, cnn_y_pred, bnn_y_pred, requested_resource_data, owner)\n",
    "    \n",
    "plot_metrics_histogram(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_histogram(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = {}\n",
    "\n",
    "for model_name, model in [('LSTM', lstm_model), ('BNN', bnn_model), ('CNN', cnn_model)]:\n",
    "    results[model_name] = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(f\"DTW Score: {metrics['DTW']}\")\n",
    "    print(f\"MAE: {metrics['MAE']}\")\n",
    "    print(f\"MAPE: {metrics['MAPE']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MULTIPLE RESOURCE PREDICTION ----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
